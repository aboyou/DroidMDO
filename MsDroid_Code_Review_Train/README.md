# **MsDroid Code Review - Training**
Ø¯Ø± ÛŒØ§Ø¯Ø¯Ø§Ø´Øª Ù‚Ø¨Ù„ÛŒ ÙØ±Ø§ÛŒÙ†Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø²ÛŒØ±Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ ([Ù„ÛŒÙ†Ú©](https://github.com/aboyou/DroidMDO/tree/main/MsDroid_Code_Review))ØŒ Ø¯Ø± Ø§ÛŒÙ† ÛŒØ§Ø¯Ø¯Ø§Ø´Øª Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ÙØ±Ø§ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù† ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.
```python
#!/usr/bin/env python

# -*- encoding: utf-8 -*-

'''
@File Â  Â : Â  train.py
@Time Â  Â : Â  2020/12/26 18:12:18
@Author Â : Â  Yiling He
@Version : Â  1.0
@Contact : Â  heyilinge0@gmail.com
@License : Â  (C)Copyright 2020
@Desc Â  Â : Â  None
'''
# here put the import lib
import sys
import os
import logging
from training import set_train_config, GraphDroid
from graph import generate_graph
from utils import makedirs, set_logger
from main import generate_behavior_subgraph

exp_base = './training/Experiments'
graph_base = f'./training/Graphs'
logger = set_logger(logging.getLogger())

if __name__ == "__main__":
Â  Â  import argparse
Â  Â  parser = argparse.ArgumentParser(description='MsDroid Trainer.')
Â  Â  # Generate behavior subgraphs
Â  Â  parser.add_argument('--input', '-i', help='APK directory')
Â  Â  parser.add_argument('--output', '-o', help='output directory', default=f'{sys.path[0]}/Output')
Â  Â  parser.add_argument('--device', '-d', help='device for model test', default='cpu')
Â  Â  parser.add_argument('--batch', '-b', help='batch size for model test', default=16)
Â  Â  parser.add_argument('--label', '-l', help='dataset label: malware(1) / benign(0), unnecessary if only prediction needed.', default=1)
Â  Â  parser.add_argument('--deepth', '-dp', help='deepth of tpl searching', default=3)
Â  Â  # Training
Â  Â  parser.add_argument(
Â  Â  '--dbs', Â  Â  Â  Â  Â  Â  Â  Â  Â  # Argument name
Â  Â  nargs='+', Â  Â  Â  Â  Â  Â  Â  Â  # Accept one or more values as a list
Â  Â  default=['TestAPK'], Â  Â  Â  # Default value when no argument is provided
Â  Â  help='Datasets to train (space-separated).' Â # Description of the argument
)
Â  Â  #parser.add_argument('--dbs', type=list, default=['TestAPK'], help='Datasets to train.')
Â  Â  parser.add_argument('--tpl', type=bool, default=True, help='TPL simplified subgraphs.')
Â  Â  parser.add_argument('--hop', type=int, default=2, help='K-hop based subgraphs.')
Â  Â  parser.add_argument('--batch_size', type=int, default=64, help='Batch size for Dataloader.')
Â  Â  parser.add_argument('--train_rate', type=float, default=0.8, help='Training rate.')
Â  Â  parser.add_argument('--norm_op', type=bool, default=False, help='Normalize opcodes feature.')
Â  Â  parser.add_argument('--mask', type=int, default=-1, help='Mask node features. 0: disable opcodes, 1: disable permission, 2: disable both')
Â  Â  parser.add_argument('--global_pool', type=str, default='mix', help='Global pooling method for graph classification.')
Â  Â  parser.add_argument('--lossfunc', type=int, default=0, help='Index of loss function.')
Â  Â  parser.add_argument('--dimension', type=int, default=128, help='Hidden layer graph embedding dimension.')
Â  Â  parser.add_argument('--dev', type=int, default=0, help='GPU device id.')
Â  Â  parser.add_argument('--exp_base', type=str, default=exp_base, help='Dir to put exp results.')
Â  Â  parser.add_argument('--graph_base', type=str, default=graph_base, help='Dir for graphs.')
Â  Â  # For Train (`train_and_test`)
Â  Â  parser.add_argument('--epoch', type=int, default=1, help='Training epoches.')
Â  Â  parser.add_argument('--force', type=bool, default=False, help='Force new train in exp_base with same config.')
Â  Â  parser.add_argument('--continue_train', type=bool, default=False, help='Continue to train from last checkpoint.')
Â  Â  args = parser.parse_args()
Â  Â  input_dir = args.input
Â  Â  apk_base = os.path.abspath(os.path.join(input_dir,'../'))
Â  Â  db_name = input_dir.split(apk_base)[-1].strip('/')
Â  Â  output_dir = args.output
Â  Â  makedirs(output_dir)
Â  Â  label = args.label
Â  Â  dbs = args.dbs
Â  Â  tpl = args.tpl
Â  Â  hop = args.hop
Â  Â  batch_size = args.batch_size
Â  Â  train_rate = args.train_rate
Â  Â  norm_opcode = args.norm_op
Â  Â  mask = args.mask
Â  Â  global_pool = args.global_pool
Â  Â  dimension = args.dimension
Â  Â  lossfunc = args.lossfunc
Â  Â  dev = args.dev
Â  Â  epoch = args.epoch
Â  Â  force = args.force
Â  Â  continue_train = args.continue_train
Â  Â  exp_dir = f'./training/Graphs/{db_name}/HOP_{hop}/TPL_{tpl}'
Â  Â  if not os.path.exists(f'{exp_dir}/dataset.pt'):
Â  Â  Â  Â  makedirs('Mappings')
Â  Â  Â  Â  import time
Â  Â  Â  Â  T1 = time.process_time() Â  Â 
Â  Â  Â  Â  '''
Â  Â  Â  Â  ./training/Graphs/<db_name>/processed/data_<apk_id>_<subgraph_id>.pt
Â  Â  Â  Â  '''
Â  Â  Â  Â  num_apk = generate_behavior_subgraph(apk_base, db_name, output_dir, args.deepth, label, hop=hop, tpl=tpl, training=True, api_map=True)
Â  Â  Â  Â  T2 = time.process_time()
Â  Â  Â  Â  print(f'Generate Behavior Subgraphs for {num_apk} APKs: {T2-T1}')
Â  Â  Â  Â  testonly = True if num_apk==1 else False

Â  Â  model_config = set_train_config(batch_size=batch_size, train_rate=train_rate, global_pool=global_pool, lossfunc=lossfunc, dimension=dimension)
Â  Â  graph_droid = GraphDroid(hop, tpl, dbs, norm_opcode=norm_opcode, mask=mask, model_config=model_config, exp_base=args.exp_base, graph_base=args.graph_base, logger=logger)
Â  Â  graph_droid.train_and_test(epoch, force=force, continue_train=continue_train, dev=dev, testonly=testonly)
```

Ú©Ø¯ Ø¨Ø§Ù„Ø§ ÙØ§ÛŒÙ„ `train.py` Ø§Ø³Øª Ú©Ù‡ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ú©Ø¯ Ø¨Ø§Ù„Ø§ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø´Ø§Ù‡Ø¯ Ø¢Ù† Ù‡Ø³ØªÛŒÙ… Ú©Ù‡ Ø²ÛŒØ±Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø±ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ù†Ø¯.
```python
num_apk = generate_behavior_subgraph(apk_base, db_name, output_dir, args.deepth, label, hop=hop, tpl=tpl, training=True, api_map=True)
```

Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø®Ø· Ø²ÛŒØ± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯:
```python
odel_config = set_train_config(batch_size=batch_size, train_rate=train_rate, global_pool=global_pool, lossfunc=lossfunc, dimension=dimension)
```

Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯. Ø¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ØŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… `training_rate` ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ú©Ù‡ Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ/ØªØ³Øª Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø«Ù„Ø§ `train_rate=0.8` Ø¨Ù‡ Ù…Ø¹Ù†Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Û¸Û°Ùª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Û²Û°Ùª Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ/ØªØ³Øª Ø§Ø³Øª.
Ù‡Ù…Ú†Ù†ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¯ÛŒÚ¯Ø± `global_pool` Ø§Ø³Øª. Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± Ù†ÙˆØ¹ Ø¹Ù…Ù„ÛŒØ§Øª pooling Ø±ÙˆÛŒ Ú¯Ø±Ø§Ù Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± `dimension`ØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø¯Ø± Ù…Ø¯Ù„ Ú¯Ø±Ø§Ù Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

# Ø¬Ø±ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ ÙØ±Ø§ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´

![DroidMDO](output(1).png)

```
                 Start
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Initialize GraphDroid Class â”‚
      â”‚ (Loads Configuration)       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          Calls `train_and_test()`
                    â”‚
                    â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Initialize Experiment Setup      â”‚
  â”‚   (Check existing training data)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Train-Test Split           â”‚
      â”‚ (Separate Training & Test) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        Calls `my_train()`
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    Train Model using PyTorch       â”‚
   â”‚    (Run for `num_epoch` epochs)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Save Best Model Weights    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        Calls `__get_scores()`
                    â”‚
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Evaluate Model Performance       â”‚
    â”‚   (Compute Precision, Recall, F1)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Generate Performance      â”‚
      â”‚  Reports & Save Results    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
       Calls `portability_test()`
                    â”‚
                    â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Test on Different Datasets    â”‚
     â”‚  (Generalization Check)        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        Calls `explain_subgraphs()`
                    â”‚
                    â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Explain Subgraph Predictions  â”‚
      â”‚ (Find Important APIs & Edges) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Generate Explanation      â”‚
      â”‚  Reports & Visualizations  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                Finished ğŸ‰

```

Ú©Ø¯ Ú©Ù„Ø§Ø³ GraphDroid Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
'''
@File    :   train_and_test.py
@Time    :   2020/07/08 20:54:38
@Author  :   Yiling He
@Version :   1.0
@Contact :   heyilinge0@gmail.com
@License :   (C)Copyright 2020
@Desc    :   Main class to train and test.
'''

# here put the import lib
import torch
import random
from torch_geometric.data import DataLoader
from tensorboardX import SummaryWriter
from datetime import datetime
import os
import pandas as pd
from training.model import my_train, my_test
from training.loader import GraphDroidDataset
from training.experiment import Experiment
from training.explainer import GraphExplainer
from training.search_emb import subgraph_cmp

import logging
from utils import add_fh, set_logger, metric2scores
logger = logging.getLogger()

exp_base = './training/Experiments'
graph_base = f'./training/Graphs'


class GraphDroid():
    def __init__(self, hop, tpl, train_dbs, norm_opcode=False, mask=-1, model_config=None, exp_base=exp_base, graph_base=graph_base, logger=logger):
        self.hop = hop
        self.tpl = tpl
        self.train_dbs = train_dbs
        self.norm_opcode = norm_opcode
        self.mask = mask
        self.model_config = model_config if model_config is not None else self.__get_basic_train_config()
        self.exp_base = exp_base
        self.graph_base = graph_base
        self.logger = logger

    def train_and_test(self, num_epoch, force=False, continue_train=False, dev=None, testonly=False, model_dict=None):
        exp = Experiment(set(self.train_dbs), self.tpl, self.hop, self.norm_opcode, self.mask, self.model_config, self.exp_base, self.graph_base, force, continue_train, model_dict)
        if not os.path.exists(exp.exp_train):
            assert not continue_train, 'Can\'t continue: train data not found.' 
            self.__train_test_split(exp, testonly=testonly)

        if num_epoch:
            train_loader = self.__torch_loader(exp.exp_train)
            test_loader = self.__torch_loader(exp.exp_test)
            global_pool = self.model_config['global_pool']
            dimension = self.model_config['dimension']
            lossfunc = self.model_config['lossfunc']
            add_fh(self.logger, exp.log_path)
            
            layer_norm = False#True if self.norm_opcode else False
            model_dict = exp.last_model if model_dict is None else model_dict
            logger.info(f'{num_epoch} epoch training from model dict {model_dict}')
            num_epoch, last_model = my_train(train_loader, test_loader, exp.writer, model_dict, dev=dev, num_epoch=num_epoch, global_pool=global_pool, lossfunc=lossfunc, dimension=dimension, start_epoch=exp.start_epoch, layer_norm=layer_norm)
            
            assert num_epoch !=0, 'no successfully trained epoch'
            exp.set_last_model(num_epoch)
            torch.save(last_model.state_dict(), exp.last_model)

        best_models = self.__prepare_models(exp)
        logger.info(f'Best F1-score models: {best_models}')

        performances = []
        for model_path in best_models:
            try:
                model, tag = exp.load_model(model_path=model_path)
            except RuntimeError:
                logger.exception(f'Failed to load corrupt model {model_path}.')
                os.remove(model_path)
                continue
            try:
                result = self.__get_scores(model, f'{exp.score_path}/{tag}.csv', data_loader=test_loader)
            except UnboundLocalError:
                print(exp.exp_test)
                test_loader = self.__torch_loader(exp.exp_test)
                result = self.__get_scores(model, f'{exp.score_path}/{tag}.csv', data_loader=test_loader)
            try:
                performances.append(tuple([float(i) for i in tag.split('_')]))
            except ValueError:
                performances.append(self.__criterion(result))
        self.__performance_report(exp, performances, 'test_performance')

    def portability_test(self, test_dbs, dev=None, models=None, testonly=False):
        if type(test_dbs) == list:
            test_dbs.sort()
            report_tag = str(test_dbs)
        test_dbs = set(test_dbs)
        train_dbs = set(self.train_dbs)
        if not testonly: assert not test_dbs.issubset(train_dbs)
        exp = Experiment(train_dbs, self.tpl, self.hop, self.norm_opcode, self.mask, self.model_config, self.exp_base, self.graph_base)
        models = self.__prepare_models(exp, models)
        if len(logger.handlers) < 2:
            add_fh(self.logger, exp.log_path)

        performances = []
        for model_path in models:
            logger.info('using model %s', model_path)
            model, tag = exp.load_model(model_path=model_path)
            tb = '+'.join(test_dbs)
            result_path = f'{exp.protest_path}/{tb}_{tag}.csv'
            result = self.__get_scores(model, result_path, dbs=test_dbs)
            performance = self.__criterion(result)
            performances.append(performance)
            logger.info(f'[portability @ {test_dbs}] {model_path}: {performance}')
        self.__performance_report(exp, performances, report_tag)

    def test_specific(self, test_db, batch_size=64, dev=None, emb_=False):
        exp = Experiment(set(self.train_dbs), self.tpl, self.hop, self.norm_opcode, self.mask, self.model_config, self.exp_base, self.graph_base)
        test_dataset = GraphDroidDataset(test_db, self.hop, self.tpl)
        test = test_dataset.get_dataset(norm_opcode=self.norm_opcode, mask=self.mask)
        test_loader = DataLoader(test, batch_size=batch_size)
        model, _ = exp.load_model(model_path=self.__prepare_models(exp)[0])
        if emb_:
            embeddings = my_test(test_loader, model.to(dev), emb_=True, dev=dev)
            return embeddings
        else:
            mapping = test_dataset.get_mapping()
            # api labels
            mapping['prediction'] = my_test(test_loader, model.to(dev), is_validation=True, dev=dev)
            return mapping[mapping.prediction==1].groupby(['graph_id', 'apk'])['api'].apply(lambda x: x.str.cat(sep=','))

    def explain_subgraphs(self, epoch=200, useclass=1, dbs=None, models=None, mode=None, dev=None, **kwargs):
        add_fh(self.logger, 'explaination.log')
        # **kwargs: apk_id, api_id, api_name (apk)
        dbs = dbs if dbs is not None else self.train_dbs
        exp = Experiment(set(self.train_dbs), self.tpl, self.hop, self.norm_opcode, self.mask, self.model_config, self.exp_base, self.graph_base)
        models = self.__prepare_models(exp, models)
        
        if self.norm_opcode:
            mode = 'norm'
        elif self.mask != -1:
            mode = 'mask'
        mask = self.mask

        for m in models:
            model, _ = exp.load_model(model_path=m)
            for d in dbs:
                dataset = GraphDroidDataset(d, self.hop, self.tpl)
                explain(d, dataset, model, mask, epoch, useclass, mode, dev, **kwargs)
    
    def __prepare_models(self, exp, models=None):
        if models is None:
            models = exp.find_best_model()
            if not models: 
                models = [exp.last_model]
        elif type(models) == str:
            models = [models]
        return models
    
    def __train_test_split(self, exp, testonly=False):
        test = []
        train = []
        logger.info('Splitting train test set.')
        for d in self.train_dbs:
            logger.debug(d)
            datas = get_dataset([d], self.hop, self.tpl, self.norm_opcode, self.mask, shuffle=True)
            data_size = len(datas)
            logger.info(f'{d}: {data_size}')
            logger.debug(f'mask: {self.mask}, e.g., {datas[0].data}')
            train_rate = self.model_config['train_rate'] 
            test += datas[int(data_size * train_rate):]
            if testonly: # `TestOnly` Dataset (to avoid ValueError since only one apk exists)
                train  += datas[int(data_size * train_rate):];continue 
            train += datas[:int(data_size * train_rate)] 
        torch.save(train, exp.exp_train)
        torch.save(test, exp.exp_test)

    def __torch_loader(self, data_path, shuffle=True):
        batch_size = self.model_config['batch_size']
        data = torch.load(data_path)
        loader = DataLoader(data, batch_size=batch_size, shuffle=True)
        return loader

    def __get_basic_train_config(self):
        return set_train_config()

    def __performance_report(self, exp, value, col_indexer):
        report = f'{exp.exp_base}/performance.csv'
        configs = pd.read_csv(exp.config_file)
        if os.path.exists(report):
            performance = pd.read_csv(report)
            configs = pd.merge(configs, performance, how='left')
        exp_index = configs[configs.exp_id==exp.exp_id].index
        configs.loc[exp_index, col_indexer] = str(value)
        configs.to_csv(report, index=False)

    def __get_scores(self, model, path, dbs=None, data_loader=None):
        result = pd.DataFrame()
        if data_loader is None:
            scores = []
            labels = []
            pretag = []
            dbname = []
            dbs = dbs if dbs is not None else self.train_dbs
            for d in dbs:
                logger.info(f'testing dataset {d}')
                score, label, plabel = self.__test_dataset(d, model)
                scores += score
                labels += label
                pretag += plabel
                dbname += [d for _ in range(len(label))]
            result['dataset'] = dbname
        else:
            scores, labels, pretag = my_test(data_loader, model, curve=True)
        result['score'] = scores
        result['label'] = labels
        result['prediction'] = pretag
        result.to_csv(path, index=False)
        logger.debug('Scores and labels are saved in %s', path) 
        return result

    def __criterion(self, result):
        TP = len(result[(result.prediction==1) & (result.label==1)])
        FP = len(result[(result.prediction==1) & (result.label==0)])
        TN = len(result[(result.prediction==0) & (result.label==0)])
        FN = len(result[(result.prediction==0) & (result.label==1)])
        return metric2scores(TP, FP, TN, FN)

    def __test_dataset(self, test_db, model, batch_size=64):
        test = GraphDroidDataset(test_db, self.hop, self.tpl).get_dataset(norm_opcode=self.norm_opcode, mask=self.mask)
        test_loader = DataLoader(test, batch_size=batch_size)
        # scores, labels, predict_label
        return my_test(test_loader, model, curve=True)


def set_train_config(batch_size=64, train_rate=0.8, global_pool='mix', lossfunc=0, dimension=128):
    config_dict = {'batch_size': batch_size, 'train_rate': train_rate, 'global_pool': global_pool, 'lossfunc':lossfunc, 'dimension': dimension}
    return config_dict


def get_dataset(dbs, hop=3, tpl=False, norm_opcode=False, mask=-1, shuffle=True, sample=None):
    if type(dbs) == str:
        dbs = [dbs]
    Datasets = []
    for d in dbs:
        data = GraphDroidDataset(d, hop, tpl)
        Datasets += data.get_dataset(norm_opcode=norm_opcode, mask=mask)
    if shuffle:
        random.shuffle(Datasets)
    if sample is not None:
        sample = int(len(Datasets)*sample) if type(sample)==float else sample
        Datasets = random.sample(Datasets, sample)
    return Datasets


def train_test_split(datas, train_rate=0.8, batch_size=64):
    data_size = len(datas)
    train = datas[:int(data_size * train_rate)]
    test = datas[int(data_size * train_rate):]
    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)
    return train_loader, test_loader


def subgraphs_explain(data, model, epoch=500, useclass=1, dev=None, search=False):
    # set `useclass` to 1: only expalin API subgraphs which are predicted as malicoius
    try: # apk
        subgraphs = data.data
    except AttributeError: # api list
        subgraphs = data
    
    suspicious_apis = []

    rbatch = DataLoader(subgraphs, batch_size=1)
    for i in rbatch:
        explainer = GraphExplainer(model, epochs=epoch)
        # edge_index = pd.DataFrame(i.edge_index.numpy().T)
        logger.debug(i.mapping[0])
        try:
            edge_mask, _ = explainer.explain_graph(i.to(dev), useclass=useclass)
            if edge_mask is not None:
                logger.debug(f'[Edge Mask]:\n{edge_mask.cpu().numpy()}')
                # edge_mask = torch.ones(edge_mask.size())
                explainer.visualize_api_subgraph(i, edge_mask)
                suspicious_apis.append(i.mapping[0][i.center[0]])
        except Exception as e:
            logger.error(e)

    logger.info(f'Suspicious APIs: {suspicious_apis}')
    if search:
        mal_cmp = subgraph_cmp(subgraphs, model)
        logger.info(f'Malware API usage similarity (Family level):\n{mal_cmp}')
        return mal_cmp
    else:
        return None


def explain(d, dataset, model, mask=-1, epoch=200, useclass=1, mode=None, dev=None, **kwargs):
    # **kwargs: apk_id, api_id, api_name (apk)
    apk_id = kwargs.get('apk_id')
    api_id = kwargs.get('api_id')
    api_name = kwargs.get('api_name')
    search = kwargs.get('search')

    if api_name is not None:
        logger.info(f'Explaining all API:{api_name} usage in {d}.')
        apk = kwargs.get('apk') or True
        subgraphs = dataset.load_db_apis(api_name, apk, mode, mask=mask)
        if apk: # apk list
            logger.debug('**APK level**')
            graphs, apks = subgraphs
            apk_num = len(apks)
            for i in range(apk_num):
                logger.info(f'{apks[i]}')
                subgraphs_explain(graphs[i], model, epoch, useclass, search=search)
            return
        # subgraphs: api list
        logger.debug(f'**API level**')

    elif api_id is not None:
        assert apk_id is not None
        logger.info(f'Explaining APK:{apk_id} API:{api_id} in {d}.')
        subgraphs = [dataset.load_api(apk_id, api_id, mode, mask=mask)]
    elif apk_id is not None:
        logger.info(f'Explaining APK:{apk_id} in {d}.')
        subgraphs = dataset.load_apk(apk_id, mode, mask=mask)

    mal_cmp = subgraphs_explain(subgraphs, model, epoch, useclass, dev, search=search)
    if mal_cmp:
        mal_cmp.to_csv(f'{apk_id}_explanation_statistics.csv', header=None, index=None)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='GraphDroid Model Trainer and Tester.')
    parser.add_argument('--dbs', type=list, default=['TestOnly'], help='Datasets to train.')
    parser.add_argument('--tpl', type=bool, default=True, help='TPL simplified subgraphs.')
    parser.add_argument('--hop', type=int, default=2, help='K-hop based subgraphs.')
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size for Dataloader.')
    parser.add_argument('--train_rate', type=float, default=0.8, help='Training rate.')
    parser.add_argument('--norm_op', type=bool, default=False, help='Normalize opcodes feature.')
    parser.add_argument('--mask', type=int, default=-1, help='Mask node features. 0: disable opcodes, 1: disable permission, 2: disable both')
    parser.add_argument('--global_pool', type=str, default='mix', help='Global pooling method for graph classification.')
    parser.add_argument('--lossfunc', type=int, default=0, help='Index of loss function.')
    parser.add_argument('--dimension', type=int, default=128, help='Hidden layer graph embedding dimension.')
    parser.add_argument('--dev', type=int, default=0, help='GPU device id.')
    parser.add_argument('--exp_base', type=str, default=exp_base, help='Dir to put exp results.')
    parser.add_argument('--graph_base', type=str, default=graph_base, help='Dir for graphs.')
    # For Train (`train_and_test`)
    parser.add_argument('--epoch', type=int, default=1000, help='Training epoches.')
    parser.add_argument('--force', type=bool, default=False, help='Force new train in exp_base with same config.')
    parser.add_argument('--continue_train', type=bool, default=False, help='Continue to train from last checkpoint.')

    args = parser.parse_args()

    dbs = args.dbs
    tpl = args.tpl
    hop = args.hop
    batch_size = args.batch_size
    train_rate = args.train_rate
    norm_opcode = args.norm_op
    mask = args.mask
    global_pool = args.global_pool
    dimension = args.dimension
    lossfunc = args.lossfunc
    dev = args.dev
    epoch = args.epoch
    force = args.force
    continue_train = args.continue_train

    hop = 2
    tpl = True
    # dbs = ['Drebin', 'Benign_Old1', 'Benign_Old2', 'Benign_Old3', 'Benign_Old4', 'amd', 'Benign_New1', 'Benign_New2', 'Benign_New3', 'Benign_New4']
    dbs = ['Drebin', 'Benign_Old']

    model_config = set_train_config(batch_size=batch_size, train_rate=train_rate, global_pool=global_pool, lossfunc=lossfunc, dimension=dimension)
    graph_droid = GraphDroid(hop, tpl, dbs, norm_opcode=norm_opcode, mask=mask, model_config=model_config, exp_base=args.exp_base, graph_base=args.graph_base)
    
    # (CUDA out of memory solution) Linux command: `export CUDA_VISIBLE_DEVICES=2,3`
    # or uncomment the following line
    # os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    # graph_droid.train_and_test(epoch, force=force, continue_train=continue_train, dev=dev) #, testonly=True #, model_dict="/opt/data/E0/GraphDroid/GraphNN_bakup/experiments/20201024-182905/models/0"
    
    # graph_droid.portability_test(['amd'], dev=dev) # dbs, testonly=True

    """
    Case Study 1: why `Reflection` good?
    Drebin/Adsms/54ece852176437e02ce2400e48c5545d32a5d69adee4a66a337ba98c67aea94e
    """
    # graph_droid.explain_subgraphs(apk_id=1469, api_id=2, epoch=500, dbs=['Drebin'])
    # graph_droid.explain_subgraphs(apk_id=1233, api_id=2, epoch=500, dbs=['Drebin_Reflection'])
    """
    Case Study 2: COVID themed (2020)
    malware_vt5: 34952977658d3ef094a505f51de73c4902265b856ec90d164a34ae178474558f   # 136
    """
    # graph_droid.explain_subgraphs(apk_id=136, epoch=500, dbs=['malware_vt5'], search=True)

    """Drebin (4 node types)"""
    graph_droid.explain_subgraphs(apk_id=5552, api_id=2, epoch=500, dbs=['Drebin'])

    # graph_droid.test_specific('malware_vt5', dev=dev, batch_size=batch_size).to_csv('malware_vt5.csv')
    # """amd-0~amd-2"""
    # for i in range(3):
    #     db = 'amd-' + str(i)
    #     graph_droid.test_specific(db, dev=dev, batch_size=batch_size).to_csv('%s.csv'%db)

    '''
    Set up malware db for 3rd level explaination.
    '''
    # for i in ['amd-0', 'amd-1', 'amd-2']:
    #     logger.debug(i)
    #     embeddings = graph_droid.test_specific(i, dev=dev, batch_size=batch_size, emb_=True)
    #     logger.debug(f'Embedding @ {i}. e.g., \n{embeddings[0]}')
    #     torch.save(embeddings, f"/opt/data/E0/GraphDroid/helper/AMD_embeddings/{i}.pt")

```

# Ø§ÙˆÙ„ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Initialization)
```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Initialize GraphDroid Class â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Load Graph Configuration Parameters                        â”‚
      â”‚ - hop, tpl, train_dbs, norm_opcode, mask                   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Set Up Model Training Configurations                        â”‚
      â”‚ - Load model_config or set default via __get_basic_train_config() â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Define Experiment & Graph Paths                            â”‚
      â”‚ - exp_base, graph_base                                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Set Up Logger                                                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```


# Ù‚Ø¯Ù… Ø¯ÙˆÙ…: ØªÙ†Ø¸ÛŒÙ…Ø§Øª experiment

Ú©Ù„Ø§Ø³ `Experiment` **ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ†** Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§: 

âœ… **Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø´ÙˆÙ†Ø¯** (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§).  
âœ… **Ù†Ù‚Ø§Ø· Ø¨Ø§Ø²Ø±Ø³ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø­ÙØ¸ Ø´ÙˆÙ†Ø¯** (Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²).  
âœ… **Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÛŒØ§ÙØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯** (Ù„Ø§Ú¯â€ŒÙ‡Ø§ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ØŒ Ù†ØªØ§ÛŒØ¬).

## Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ø²Ù…Ø§ÛŒØ´
```python
class Experiment():
    def __init__(self, trained_db, tpl, hop, norm_opcode, mask, model_config, exp_base=exp_base, graph_base=graph_base, force=False, continue_train=False, model_dict=None):
```

- Ø§Ø·Ù„Ø§Ø¹Ø§Øª **Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú¯Ø±Ø§Ù Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´** Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ **Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª**.
- Ø§Ú¯Ø± `continue_train=True` Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù…ÙˆØ²Ø´ **Ø§Ø² Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯**.

## ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ (`()set_paths__`)
```python
def __set_paths(self):
    makedirs(self.exp_base)
    self.config_file = f'{self.exp_base}/exp_configs.csv'
    columns = ['exp_id', 'trained_db', 'tpl', 'hop', 'norm_opcode', 'mask', 'model_config']
    if not os.path.exists(self.config_file):
        (pd.DataFrame([], columns=columns)).to_csv(self.config_file, index=False)
```

- Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ (`exp_base`) Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
- ÛŒÚ© **ÙØ§ÛŒÙ„ CSV (`exp_configs.csv`)** Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

## Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…Ø´Ø§Ø¨Ù‡ (()get_exp_id__`)

```python
def __get_exp_id(self):
    configs = pd.read_csv(self.config_file)
    exp_id = None
    for _, v in configs.iterrows():
        match = (eval(v.trained_db) == self.trained_db) & (v.tpl == self.tpl) & (v.hop == self.hop) & (v.norm_opcode == self.norm_opcode) & (v['mask'] == self.mask) & (eval(v.model_config) == self.model_config)
        if match:
            exp_id = v.exp_id
            break
    if exp_id is not None:
        self.exp_id = exp_id
```

- ÙØ§ÛŒÙ„ `exp_configs.csv` Ø±Ø§ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ **Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª**.
- Ø§Ú¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ù‚Ø¯Ø§Ø± **`exp_id`** Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

## Ø§ÛŒØ¬Ø§Ø¯ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù‚Ø¨Ù„ÛŒ
```python
if self.exp_id is None or (self.force and not self.continue_train):
    self.__create_exp(columns)
```

- Ø§Ú¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´ Ù‚Ø¨Ù„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´ÙˆØ¯ØŒ ÛŒÚ© **Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯**.
- Ø§Ú¯Ø± `force=True` Ø¨Ø§Ø´Ø¯ØŒ **Ø¢Ø²Ù…Ø§ÛŒØ´ Ù‚Ø¨Ù„ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯**.

## Ø§Ø®ØªØµØ§Øµ Ø´Ù†Ø§Ø³Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´ (()create_exp__`)
```python
def __create_exp(self, columns):
    setup_logger.info('Creating a new experiment.')
    self.exp_id = datetime.now().strftime("%Y%m%d-%H%M%S")  # Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§
    configs = pd.DataFrame([[self.exp_id, self.trained_db, self.tpl, self.hop, self.norm_opcode, self.mask, self.model_config]], columns=columns)
    configs.to_csv(self.config_file, mode='a', header=False, index=False)
```

- ÛŒÚ© **Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯**.
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ **Ø¯Ø± `exp_configs.csv` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯**.

## ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ†
```python
exp_data = f'{self.exp_base}/{self.exp_id}/TrainTest'
makedirs(exp_data)
self.exp_train = f'{exp_data}/train.pt'
self.exp_test = f'{exp_data}/test.pt'
self.log_path = f'{self.exp_base}/{self.exp_id}/exp_log.log'
```

Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡:

- **Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ†** (`train.pt`, `test.pt`)
- **Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´** (`exp_log.log`) ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

## Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
```python
self.model_path = f'{self.exp_base}/{self.exp_id}/models'
self.get_models(last=True, model_dict=self.model_dict)
if self.continue_train:
    if self.last_model.split('/')[-1] != '0':
        self.start_epoch = self.__epoch_from_log() + 1
    else:
        clear = True
        ori_log = f'{self.exp_base}/{self.exp_id}/exp_log.log'
        if os.path.exists(ori_log):
            os.remove(ori_log)
    self.__get_train_paths(clear=clear)
```

- ÛŒÚ© Ù…Ø³ÛŒØ± Ø¨Ø±Ø§ÛŒ **Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡** Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ø§Ú¯Ø± `continue_train=True` Ø¨Ø§Ø´Ø¯ØŒ **Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯**.


# Ù‚Ø¯Ù… Ø³ÙˆÙ…: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ† (`__train_test_split`)

Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ train,pt Ùˆ test.pt ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø§Ø¬Ø±Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.
Ø¨Ø¹Ø¯ Ø§Ø² **Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ (Experiment Setup)**ØŒ Ø³ÛŒØ³ØªÙ… **Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ† ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯** ØªØ§ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù…Ø§Ø¯Ù‡ Ø´ÙˆØ¯.
```python
def __train_test_split(self, exp, testonly=False):
    test = []
    train = []
    logger.info('Splitting train test set.')

    for d in self.train_dbs:
        logger.debug(d)
        datas = get_dataset([d], self.hop, self.tpl, self.norm_opcode, self.mask, shuffle=True)
        data_size = len(datas)
        logger.info(f'{d}: {data_size}')
        logger.debug(f'mask: {self.mask}, e.g., {datas[0].data}')

        train_rate = self.model_config['train_rate'] 
        test += datas[int(data_size * train_rate):]
        
        if testonly:  # Ø§Ú¯Ø± `testonly=True` Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ù…Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
            train += datas[int(data_size * train_rate):]
            continue  

        train += datas[:int(data_size * train_rate)]  # ØªØ®ØµÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´

    torch.save(train, exp.exp_train)
    torch.save(test, exp.exp_test)
```
![tartin_test_split_flow](train_test_split.png)
## Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ
```python
for d in self.train_dbs:
    logger.debug(d)
    datas = get_dataset([d], self.hop, self.tpl, self.norm_opcode, self.mask, shuffle=True)
```
- Ø±ÙˆÛŒ **Ù‡Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡** Ø¯Ø± `self.train_dbs` Ø­Ù„Ù‚Ù‡ Ù…ÛŒâ€ŒØ²Ù†Ø¯.
- ØªØ§Ø¨Ø¹ **`get_dataset()`** Ø±Ø§ ØµØ¯Ø§ Ù…ÛŒâ€ŒØ²Ù†Ø¯ Ú©Ù‡:
    - **Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯** (`d`).
    - **ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯**.
    - **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯**.

## ØªØ®ØµÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†
```python
test += datas[int(data_size * train_rate):]
```
**Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡** (20%) Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª** Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
## âœ… ØªØ¹Ø±ÛŒÙ Ø¯Ù‚ÛŒÙ‚ ØªØ§Ø¨Ø¹ `get_dataset`

```python
def get_dataset(dbs, hop=3, tpl=False, norm_opcode=False, mask=-1, shuffle=True, sample=None):
    if type(dbs) == str:
        dbs = [dbs]
    Datasets = []
    for d in dbs:
        data = GraphDroidDataset(d, hop, tpl)
        Datasets += data.get_dataset(norm_opcode=norm_opcode, mask=mask)
    if shuffle:
        random.shuffle(Datasets)
    if sample is not None:
        sample = int(len(Datasets)*sample) if type(sample)==float else sample
        Datasets = random.sample(Datasets, sample)
    return Datasets
```
## ğŸ§  Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡

|Ú¯Ø§Ù…|ØªÙˆØ¶ÛŒØ­|
|---|---|
|1ï¸âƒ£|Ø§Ú¯Ø± `dbs` ÛŒÚ© Ø±Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ Ù„ÛŒØ³Øª ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.|
|2ï¸âƒ£|Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ØŒ ÛŒÚ© Ø´ÛŒ Ø§Ø² `GraphDroidDataset` Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.|
|3ï¸âƒ£|Ø¨Ø§ Ù…ØªØ¯ `get_dataset()` Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ (Ù†ÙˆØ¹ `Data`) Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.|
|4ï¸âƒ£|Ø§Ú¯Ø± `shuffle=True`ØŒ ØªØ±ØªÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØµØ§Ø¯ÙÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.|
|5ï¸âƒ£|Ø§Ú¯Ø± `sample` Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¢Ù†ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.|
|âœ…|Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.|

---
# Ù‚Ø¯Ù… Ú†Ù‡Ø§Ø±Ù…: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
Ø§ÛŒÙ† Ø®Ø· Ú©Ø¯:

```python
num_epoch, last_model = my_train(train_loader, test_loader, exp.writer, model_dict,
                                 dev=dev, num_epoch=num_epoch, global_pool=global_pool,
                                 lossfunc=lossfunc, dimension=dimension,
                                 start_epoch=exp.start_epoch, layer_norm=layer_norm)
```

ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ **Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú¯Ø±Ø§ÙÛŒ (GNN)** Ø±Ø§ Ø¯Ø§Ø±Ø¯ Ùˆ ØªØ§Ø¨Ø¹ `my_train(...)` Ø±Ø§ ØµØ¯Ø§ Ù…ÛŒâ€ŒØ²Ù†Ø¯ Ú©Ù‡ ÛŒÚ©ÛŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphDroid Ø§Ø³Øª.

---

## âœ… ÙˆØ¸ÛŒÙÙ‡ Ú©Ù„ÛŒ:

ØªØ§Ø¨Ø¹ `my_train(...)`:

- Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ `train_loader` Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
- Ø¯Ø± Ù‡Ø± epoch Ø¨Ø§ `test_loader` Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
- Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
- Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ `epoch` Ùˆ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.

---
## ğŸ§  ØªÙˆØ¶ÛŒØ­ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:

| Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†        | ØªÙˆØ¶ÛŒØ­                                                    |
| -------------- | -------------------------------------------------------- |
| `train_loader` | Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ API)                   |
| `test_loader`  | Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´                   |
| `exp.writer`   | Ø´ÛŒØ¡ `SummaryWriter` Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ TensorBoard         |
| `model_dict`   | Ù…Ø¯Ù„ Ø§ÙˆÙ„ÛŒÙ‡ (ÛŒØ§ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø² Ù‚Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´)     |
| `dev`          | Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø³ØªÚ¯Ø§Ù‡ GPU                                         |
| `num_epoch`    | ØªØ¹Ø¯Ø§Ø¯ epochÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´                        |
| `global_pool`  | Ø±ÙˆØ´ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ meanØŒ maxØŒ ÛŒØ§ mix)      |
| `lossfunc`     | Ù†ÙˆØ¹ ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ (Ù…Ø«Ù„Ø§Ù‹ CrossEntropy)                        |
| `dimension`    | ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ embedding                           |
| `start_epoch`  | Ø¯Ø± ØµÙˆØ±Øª Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø² Ø§ÛŒÙ† epoch Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯            |
| `layer_norm`   | Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ØŒ normalization Ø±ÙˆÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ |

---
## ğŸŸ¢ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§:

| Ø®Ø±ÙˆØ¬ÛŒ        | ØªÙˆØ¶ÛŒØ­                                                              |
| ------------ | ------------------------------------------------------------------ |
| `num_epoch`  | ØªØ¹Ø¯Ø§Ø¯ epochÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù†Ø¯ (Ù…Ù…Ú©Ù† Ø§Ø³Øª early-stopping Ø¨Ø§Ø´Ø¯) |
| `last_model` | Ø´ÛŒØ¡ Ù…Ø¯Ù„ PyTorch Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ (Ø¢Ø®Ø±ÛŒÙ† ÙˆØ¶Ø¹ÛŒØª)                           |

---
## ğŸ“¦ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ:

ÙØ±Ø¶ Ú©Ù†ÛŒÙ… Ø¯Ø§Ø±ÛŒÙ… Û±Û°Û°Û° Ú¯Ø±Ø§Ù APIØŒ Ù…Ø¯Ù„ Ù…Ø§ ÛŒÚ© GCN Ø§Ø³Øª Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¢Ù† Ø±Ø§ Ø±ÙˆÛŒ GPU Ø´Ù…Ø§Ø±Ù‡ 0 Ø¨Ø±Ø§ÛŒ 200 epoch Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±Øª:

- `train_loader` Ø´Ø§Ù…Ù„ Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ `Data(x=..., edge_index=...)` Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.
    
- Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ØŒ Ù‡Ø± epoch Ø´Ø§Ù…Ù„:
    - ÙÙˆØ±ÙˆØ§Ø±Ø¯ Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
    - Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
    - backpropagation
    - Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ `test_loader`
- `last_model` Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…Ø§Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØªØ³Øª Ø§Ø³Øª.

## Ù„Ø§ÛŒÛ€ GNNStack
![GNNMSDroid](GNN_MsDroid.png)
